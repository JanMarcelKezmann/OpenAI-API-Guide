{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1DS0XJkO0Bc8MYpVY6JF-sJGHHrMxfzfN",
      "authorship_tag": "ABX9TyNNjny4vfT+lxVJpzHCfu/X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JanMarcelKezmann/OpenAI-API-Guide/blob/main/Step_by_Step_Guide_ChatGPT_Python_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step-by-Step Guide ChatGPT Python API\n",
        "\n",
        "Hi, this notebook should offer you a simple ready-to-use hands-on guide for utilizing the ChatGPT API in a Jupyter Notebook.\n",
        "\n",
        "I have explained all the necessary steps in my Blog Post <a href=\"\">Your 5-Minute Step-By-Step ChatGPT API Guide</a>.\n",
        "\n",
        "This notebooks contains all the code mentioned in the blog article with additional comments from my side.\n",
        "\n",
        "I hope you find it valueable.\n"
      ],
      "metadata": {
        "id": "RoAimVPu0MIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "0qI4qQLM00nm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First install the *openai* library in this notebook."
      ],
      "metadata": {
        "id": "Jo6b3AwO1D6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00X1OkvG1BFu",
        "outputId": "74e478fc-665e-4b83-8be0-8ac02cb76552"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.9/dist-packages (0.27.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.3.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After you have installed it just import it as shown below."
      ],
      "metadata": {
        "id": "iW5gSJtr1JtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai"
      ],
      "metadata": {
        "id": "GYcn9r8f0LI7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the relevant packages are imported we can load and set the OpenAI API key from a text file.\n",
        "\n",
        "I have stored it in a file called *openai-api-key.txt* in the same folder as the Jupyter Notebook itself. (If you are working in Google Colab as I did, then you have to mount your drive first to access the files in your Google Drive.)\n",
        "\n",
        "Change the *file_path* variable if you have named your text file differently."
      ],
      "metadata": {
        "id": "lPRqx_oE1UHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your OpenAI API key from the text file\n",
        "file_path = \"drive/MyDrive/Blogs/ChatGPT API/openai-api-key.txt\"\n",
        "\n",
        "with open(file_path, \"r\") as f:\n",
        "  openai.api_key = f.read().strip(\"\\n\")"
      ],
      "metadata": {
        "id": "zfODExpd025l"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding The OpenAI ChatGPT API In A Jupyter Notebook"
      ],
      "metadata": {
        "id": "nZrz0Mx12FSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having set up the mandatory openai library and API key, we can now continue by making a test request to check whether everything is working properly.\n",
        "\n",
        "Before being able to retrieve a response, we have to define the model first. In our case we would like to utilize the **gpt-3.5-turbo** model.\n",
        "\n",
        "If you want to get a list of all models than just execute the code cell below"
      ],
      "metadata": {
        "id": "I7VOW3hd2G6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for model in openai.Model.list()[\"data\"]:\n",
        "    print(model.get(\"id\", \"---\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjMZX1mL32ts",
        "outputId": "2c97fb32-5c20-457f-ae83-2e12fab5cc39"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "babbage\n",
            "davinci\n",
            "text-davinci-edit-001\n",
            "babbage-code-search-code\n",
            "text-similarity-babbage-001\n",
            "code-davinci-edit-001\n",
            "text-davinci-001\n",
            "ada\n",
            "babbage-code-search-text\n",
            "babbage-similarity\n",
            "code-search-babbage-text-001\n",
            "text-curie-001\n",
            "whisper-1\n",
            "code-search-babbage-code-001\n",
            "text-davinci-003\n",
            "text-ada-001\n",
            "text-embedding-ada-002\n",
            "text-similarity-ada-001\n",
            "curie-instruct-beta\n",
            "ada-code-search-code\n",
            "ada-similarity\n",
            "code-search-ada-text-001\n",
            "text-search-ada-query-001\n",
            "davinci-search-document\n",
            "ada-code-search-text\n",
            "text-search-ada-doc-001\n",
            "davinci-instruct-beta\n",
            "text-similarity-curie-001\n",
            "code-search-ada-code-001\n",
            "ada-search-query\n",
            "text-search-davinci-query-001\n",
            "curie-search-query\n",
            "davinci-search-query\n",
            "babbage-search-document\n",
            "ada-search-document\n",
            "text-search-curie-query-001\n",
            "text-search-babbage-doc-001\n",
            "curie-search-document\n",
            "text-search-curie-doc-001\n",
            "babbage-search-query\n",
            "text-babbage-001\n",
            "text-search-davinci-doc-001\n",
            "text-search-babbage-query-001\n",
            "curie-similarity\n",
            "gpt-3.5-turbo-0301\n",
            "curie\n",
            "gpt-3.5-turbo\n",
            "text-similarity-davinci-001\n",
            "text-davinci-002\n",
            "davinci-similarity\n",
            "cushman:2020-05-03\n",
            "ada:2020-05-03\n",
            "babbage:2020-05-03\n",
            "curie:2020-05-03\n",
            "davinci:2020-05-03\n",
            "if-davinci-v2\n",
            "if-curie-v2\n",
            "if-davinci:3.0.0\n",
            "davinci-if:3.0.0\n",
            "davinci-instruct-beta:2.0.0\n",
            "text-ada:001\n",
            "text-davinci:001\n",
            "text-curie:001\n",
            "text-babbage:001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Accessing the gpt-3.5-turbo model costs $0.002 per 1,000 tokens."
      ],
      "metadata": {
        "id": "Z8_y20Vd4ekX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"gpt-3.5-turbo\""
      ],
      "metadata": {
        "id": "CY43mQJt4YIr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define A message"
      ],
      "metadata": {
        "id": "sIHvB9aN6lb5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crafting a message requires to steps:\n",
        "\n",
        "1. Define the **role**\n",
        "2. Define the **content**\n",
        "\n",
        "The **role** can be one of three things: *system*, *user*, *assistant*\n",
        "\n",
        "A short explanation:\n",
        "\n",
        " - *system*: Primes the chatbot to act in a specific way, e.g., 'act as a helpful mathematics professor'\n",
        " - *user*: This is basically *you*, and is used for all queries you make\n",
        " - *assistant*: This is a previous response of the chatbot, useful for conversations in which you might refer to the previous outputs"
      ],
      "metadata": {
        "id": "2gDBvG726pzv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our first example, we just want to query a response by asksing in the role of the *user*."
      ],
      "metadata": {
        "id": "aFD254Xs8dva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_first_message = \"How many people live in Brasil?\"\n",
        "\n",
        "message_list = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": my_first_message\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "SsaEaRkv6ocJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get The Response"
      ],
      "metadata": {
        "id": "By0oXmgEKp42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "completion = openai.ChatCompletion.create(\n",
        "    model=model,\n",
        "    messages=message_list\n",
        ")"
      ],
      "metadata": {
        "id": "rXX0DjJO844i"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we received no error message the query should have been successful. The complete response looks as follows:"
      ],
      "metadata": {
        "id": "hCLtgjuw9Vzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "completion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAHo86LW8_pY",
        "outputId": "08d29565-8dcb-4581-80b7-beecf61ac4cc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject chat.completion id=chatcmpl-73i0L56JFDWj6E3FIHQEphCvGgmOE at 0x7faee8fa1400> JSON: {\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"finish_reason\": \"stop\",\n",
              "      \"index\": 0,\n",
              "      \"message\": {\n",
              "        \"content\": \"As an AI language model, I do not have access to real-time data, but according to the United Nations (as of 2021), the estimated population of Brazil is 213.4 million.\",\n",
              "        \"role\": \"assistant\"\n",
              "      }\n",
              "    }\n",
              "  ],\n",
              "  \"created\": 1681118493,\n",
              "  \"id\": \"chatcmpl-73i0L56JFDWj6E3FIHQEphCvGgmOE\",\n",
              "  \"model\": \"gpt-3.5-turbo-0301\",\n",
              "  \"object\": \"chat.completion\",\n",
              "  \"usage\": {\n",
              "    \"completion_tokens\": 41,\n",
              "    \"prompt_tokens\": 15,\n",
              "    \"total_tokens\": 56\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, we are mostly interested into the answer we asked for, which you simply retrieve by executing the following line of code:"
      ],
      "metadata": {
        "id": "7sJkW3au9ZbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = completion[\"choices\"][0][\"message\"][\"content\"]\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1VIzjhh3S8i",
        "outputId": "4b7a7848-0ee4-4904-f70a-a257e05fcfc6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As an AI language model, I do not have access to real-time data, but according to the United Nations (as of 2021), the estimated population of Brazil is 213.4 million.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** As you may have noticed we receive some additional valuable information in the response as well:\n",
        "\n",
        " - **role**: assistant (aka. the chatbot)\n",
        " - **model**: gpt-3.5-turbo-0301 (the specific model model name/id)\n",
        " - **usage**: a dictionary (relevant for calcualting the price of your queries):\n",
        "    - *prompt_tokens*: the amount of tokens used in your query\n",
        "    - *completion_tokens*: the amount of tokens used in the response\n",
        "    - *total_token*: the total number of tokens"
      ],
      "metadata": {
        "id": "6xSAst079x2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And that is it for the start.\n",
        "\n",
        "Below I will provide you with a more sophisticated code that allows you to have a real chat-like conversation in your Jupyter Notebook or an application of your choice."
      ],
      "metadata": {
        "id": "uo0ESyc3_Lc_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up a Chat"
      ],
      "metadata": {
        "id": "uh-wkNJI_X6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up a chat actually won't require much extra work.\n",
        "\n",
        "What we basically need to add is the option to provide the whole chat history as a message in order for the model to understand and know the whole context.\n",
        "\n",
        "To do this I will use the *chat_history* variable - define below - to keep track of all the queries I made and responses I have received. Each time I ask something new or get a response I will append that content with the proper **role** to the list as a dictionary."
      ],
      "metadata": {
        "id": "hRmKtoJRS9hP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []"
      ],
      "metadata": {
        "id": "FWwoydnyS84m"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(user_input: str, chat_history: list = [], role: str = \"user\", model: str = \"gpt-3.5-turbo\") -> list:\n",
        "    # add your new query to the chat history\n",
        "    chat_history.append(\n",
        "        {\n",
        "            \"role\": role,\n",
        "            \"content\": user_input\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # post a request to the openai API\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=chat_history\n",
        "    )\n",
        "\n",
        "    # get the response\n",
        "    response = completion[\"choices\"][0][\"message\"][\"content\"]   \n",
        "\n",
        "    # append the response with the role - assistant - to the chat history\n",
        "    chat_history.append(\n",
        "        {\n",
        "            \"role\": completion[\"choices\"][0][\"message\"][\"role\"],\n",
        "            \"content\": response\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # return the response and chat_history\n",
        "    return response, chat_history"
      ],
      "metadata": {
        "id": "UyoSFebY3S5S"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start chatting"
      ],
      "metadata": {
        "id": "pAA6cWPgZQYn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prime the model with a system message"
      ],
      "metadata": {
        "id": "kofIzJt0ZXiW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having defined the chat function, I would test it with an example making asking two consecutive questions where the second relies on the context of the first and its response.\n",
        "\n",
        "In addition, we will prime the model by using the *system* role, to hopefully get a better response."
      ],
      "metadata": {
        "id": "PcihMA17VUKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setting the role to system to prime the model\n",
        "response, chat_history = chat(\n",
        "    user_input = \"Act as a helpful math teacher.\",\n",
        "    chat_history = chat_history,\n",
        "    role = \"system\"\n",
        ")"
      ],
      "metadata": {
        "id": "Jpdw59nZ3S1z"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's print the response and chat history to see if everything worked as expected."
      ],
      "metadata": {
        "id": "Baa9d3pEWWyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZlxAdmX3SXl",
        "outputId": "e973d62b-d576-45be-e60c-400ff60a5369"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Of course, I am here to help with any math questions you may have. What do you need help with?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UB9WVXdgWUaw",
        "outputId": "f68f22df-7568-456a-e2c9-1515557cce67"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system', 'content': 'Act as a helpful math teacher.'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Of course, I am here to help with any math questions you may have. What do you need help with?'}]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### My Initial Question"
      ],
      "metadata": {
        "id": "4K24yFLkZb71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The response as well as the chat_history looks as we wanted it, let's see if the AI model behaves as it was advised to."
      ],
      "metadata": {
        "id": "AL9xp934Wb1b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "84xyKPeuMvwm"
      },
      "outputs": [],
      "source": [
        "response, chat_history = chat(\n",
        "    user_input = \"What is the derivative of the function f(x) = x * e^x\",\n",
        "    chat_history = chat_history\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItofzPyoW2VG",
        "outputId": "cb97638c-9a41-46bd-8476-91cf940d0eb1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To find the derivative of the function f(x) = x * e^x, we can use the product rule for differentiation, which states that the derivative of a product of two functions is equal to the first function times the derivative of the second function plus the second function times the derivative of the first function.\n",
            "\n",
            "In this case, let u = x and v = e^x. Then, using the product rule, we get:\n",
            "\n",
            "f'(x) = (u)' * v + u * (v)'\n",
            "\n",
            "Where (u)' and (v)' are the derivatives of u and v respectively.\n",
            "\n",
            "Taking the derivatives, we get:\n",
            "\n",
            "f'(x) = 1 * e^x + x * e^x = (x + 1) * e^x\n",
            "\n",
            "Therefore, the derivative of the function f(x) = x * e^x is (x + 1) * e^x.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, so far this not only looks factually and mathematically correct but the model also did what it should, meaning acting as a helpful math teacher.\n",
        "\n",
        "By explaining each step which lead to the correct answer it did that perfectly."
      ],
      "metadata": {
        "id": "BfUZVcCiW8LT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Follow-Up Question"
      ],
      "metadata": {
        "id": "Xc2d4UUJZio_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if it can answer my follow up question which will require the understanding of the context from the previous question and respone."
      ],
      "metadata": {
        "id": "VwECID62ZilS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response, chat_history = chat(\n",
        "    user_input = \"Can you now reverse your last calculation\",\n",
        "    chat_history = chat_history\n",
        ")"
      ],
      "metadata": {
        "id": "4vdabEQlW4cT"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJvJTp2fXlsl",
        "outputId": "a6aee01b-c2c0-4583-ed87-a15e3c651797"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, I can do that. \n",
            "\n",
            "To reverse the previous calculation, we need to find the antiderivative (or integral) of (x + 1) * e^x with respect to x. \n",
            "\n",
            "The antiderivative is found by using integration by parts, which is the opposite of the product rule for differentiation. \n",
            "\n",
            "Let u = x + 1 and dv/dx = e^x, so that du/dx = 1 and v = e^x. \n",
            "\n",
            "Then, using integration by parts, we have: \n",
            "\n",
            "∫ (x + 1) * e^x dx = u * v - ∫ v * du/dx dx \n",
            "\n",
            "= (x + 1) * e^x - ∫ 1 * e^x dx \n",
            "\n",
            "= (x + 1) * e^x - e^x + C \n",
            "\n",
            "where C is the constant of integration. \n",
            "\n",
            "Therefore, the antiderivative of (x + 1) * e^x with respect to x is (x + 1) * e^x - e^x + C.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all, the AI model understood the context and figured out that I wanted to integrate the function that has been previously derived.\n",
        "\n",
        "Also, the model did everything correct even though it could have summarized the final sum (x + 1) * e^x - e^x + C to x * e^x + C.\n",
        "\n",
        "I could now ask to correct itself, but I will stop for now as I have just shown how to set up the chat functionality which was the main purpose of this guide."
      ],
      "metadata": {
        "id": "aMyLEqxaYLlX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's look at the compelte chat history:"
      ],
      "metadata": {
        "id": "wTTScTpZcrxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for el in chat_history:\n",
        "    str_content = el[\"content\"].replace(\"\\n\", \"\\n            | \")\n",
        "    print(f'{el[\"role\"].capitalize()}:{\" \" * (11 - len(el[\"role\"]))}| {str_content}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2CvM3V9YwHR",
        "outputId": "1e046521-a4f7-4a62-8861-07a55d0211f0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System:     | Act as a helpful math teacher.\n",
            "\n",
            "Assistant:  | Of course, I am here to help with any math questions you may have. What do you need help with?\n",
            "\n",
            "User:       | What is the derivative of the function f(x) = x * e^x\n",
            "\n",
            "Assistant:  | To find the derivative of the function f(x) = x * e^x, we can use the product rule for differentiation, which states that the derivative of a product of two functions is equal to the first function times the derivative of the second function plus the second function times the derivative of the first function.\n",
            "            | \n",
            "            | In this case, let u = x and v = e^x. Then, using the product rule, we get:\n",
            "            | \n",
            "            | f'(x) = (u)' * v + u * (v)'\n",
            "            | \n",
            "            | Where (u)' and (v)' are the derivatives of u and v respectively.\n",
            "            | \n",
            "            | Taking the derivatives, we get:\n",
            "            | \n",
            "            | f'(x) = 1 * e^x + x * e^x = (x + 1) * e^x\n",
            "            | \n",
            "            | Therefore, the derivative of the function f(x) = x * e^x is (x + 1) * e^x.\n",
            "\n",
            "User:       | Can you now reverse your last calculation\n",
            "\n",
            "Assistant:  | Sure, I can do that. \n",
            "            | \n",
            "            | To reverse the previous calculation, we need to find the antiderivative (or integral) of (x + 1) * e^x with respect to x. \n",
            "            | \n",
            "            | The antiderivative is found by using integration by parts, which is the opposite of the product rule for differentiation. \n",
            "            | \n",
            "            | Let u = x + 1 and dv/dx = e^x, so that du/dx = 1 and v = e^x. \n",
            "            | \n",
            "            | Then, using integration by parts, we have: \n",
            "            | \n",
            "            | ∫ (x + 1) * e^x dx = u * v - ∫ v * du/dx dx \n",
            "            | \n",
            "            | = (x + 1) * e^x - ∫ 1 * e^x dx \n",
            "            | \n",
            "            | = (x + 1) * e^x - e^x + C \n",
            "            | \n",
            "            | where C is the constant of integration. \n",
            "            | \n",
            "            | Therefore, the antiderivative of (x + 1) * e^x with respect to x is (x + 1) * e^x - e^x + C.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ag6sjKRdTWC"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}